{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlopRank\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ## Configuration\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
    "    model_names: List[str]\n",
    "    evaluation_method: int  # 1 for numeric, 2 for ranking\n",
    "    use_subset_evaluation: bool\n",
    "    evaluators_subset_size: int\n",
    "    output_dir: Path\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if self.evaluation_method not in {1, 2}:\n",
    "            raise ValueError(\"evaluation_method must be 1 or 2\")\n",
    "        if self.evaluators_subset_size >= len(self.model_names):\n",
    "            raise ValueError(\"evaluators_subset_size must be less than number of models\")\n",
    "\n",
    "# Default configuration\n",
    "DEFAULT_CONFIG = EvalConfig(\n",
    "    model_names=[\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-exp-1206\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"claude-3-opus-latest\",\n",
    "        \"o1-preview\",\n",
    "        \"gpt-4o\",\n",
    "        \"deepseek-chat\"\n",
    "    ],\n",
    "    evaluation_method=1,\n",
    "    use_subset_evaluation=True,\n",
    "    evaluators_subset_size=3,\n",
    "    output_dir=Path(\"results\")\n",
    ")\n",
    "\n",
    "# ## Core Evaluation Classes\n",
    "class ResponseManager:\n",
    "    \"\"\"Handles model response collection and validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvalConfig):\n",
    "        self.config = config\n",
    "        self.responses_df = None\n",
    "        \n",
    "    def _validate_response(self, response: str) -> bool:\n",
    "        \"\"\"Basic validation of model responses.\"\"\"\n",
    "        if not isinstance(response, str):\n",
    "            return False\n",
    "        if len(response.strip()) < 10:  # Arbitrary minimum length\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def collect_responses(self, prompts: List[str], llm_module) -> pd.DataFrame:\n",
    "        \"\"\"Collect responses from all models for given prompts.\"\"\"\n",
    "        responses = []\n",
    "        total_start = time.time()\n",
    "        \n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            logger.info(f\"Processing prompt {i}/{len(prompts)}\")\n",
    "            for model_name in self.config.model_names:\n",
    "                start_time = time.time()\n",
    "                logger.info(f\"Querying {model_name}...\")\n",
    "                try:\n",
    "                    model = llm_module.get_model(model_name)\n",
    "                    response = model.prompt(prompt).text()\n",
    "                    is_valid = self._validate_response(response)\n",
    "                    elapsed = time.time() - start_time\n",
    "                    \n",
    "                    logger.info(f\"{model_name} responded in {elapsed:.2f}s - \" + \n",
    "                              f\"{'Valid' if is_valid else 'Invalid'} response\")\n",
    "                    \n",
    "                    responses.append({\n",
    "                        'prompt': prompt,\n",
    "                        'model': model_name,\n",
    "                        'response': response if is_valid else None,\n",
    "                        'is_valid': is_valid,\n",
    "                        'response_time': elapsed\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    logger.error(f\"Error from {model_name} after {elapsed:.2f}s: {str(e)}\")\n",
    "                    responses.append({\n",
    "                        'prompt': prompt,\n",
    "                        'model': model_name,\n",
    "                        'response': None,\n",
    "                        'is_valid': False,\n",
    "                        'response_time': elapsed\n",
    "                    })\n",
    "                    \n",
    "                # Add a small delay between requests to avoid rate limits\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        total_time = time.time() - total_start\n",
    "        logger.info(f\"All responses collected in {total_time:.2f}s\")\n",
    "        \n",
    "        self.responses_df = pd.DataFrame(responses)\n",
    "        return self.responses_df\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Handles the evaluation of model responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvalConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def _create_evaluation_prompt(self, \n",
    "                                prompt: str, \n",
    "                                responses: Dict[str, str]) -> Tuple[str, Dict[str, str]]:\n",
    "        \"\"\"Creates the evaluation prompt and model mappings.\"\"\"\n",
    "        model_to_anonymous = {\n",
    "            model: f\"Model_{i+1}\" \n",
    "            for i, model in enumerate(responses.keys())\n",
    "        }\n",
    "        \n",
    "        answers_section = \"\\n\".join([\n",
    "            f\"{model_to_anonymous[model]}:\\n{response}\\n---\" \n",
    "            for model, response in responses.items()\n",
    "        ])\n",
    "        \n",
    "        if self.config.evaluation_method == 1:\n",
    "            instructions = f\"\"\"IMPORTANT: Return only a JSON object with ratings.\n",
    "            \n",
    "            Rate these responses to: \"{prompt}\"\n",
    "\n",
    "            {answers_section}\n",
    "\n",
    "            Rate 1-10 based on: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "            10: Exceptional, 8-9: Excellent, 6-7: Good, 4-5: Fair, 1-3: Poor\n",
    "\n",
    "            Format: {{\"Model_1\": 8, \"Model_2\": 7}}\"\"\"\n",
    "        else:\n",
    "            instructions = f\"\"\"IMPORTANT: Return only a JSON object with rankings.\n",
    "            \n",
    "            Rank these responses to: \"{prompt}\"\n",
    "\n",
    "            {answers_section}\n",
    "\n",
    "            Rank from best (1) to worst. No ties allowed.\n",
    "            Consider: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "\n",
    "            Format: {{\"Model_1\": 1, \"Model_2\": 2}}\"\"\"\n",
    "            \n",
    "        return instructions.strip(), model_to_anonymous\n",
    "    \n",
    "    def _parse_evaluation(self, \n",
    "                         raw_judgment: str, \n",
    "                         model_mapping: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"Parse and validate evaluation responses.\"\"\"\n",
    "        try:\n",
    "            # Extract JSON object\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "            if start == -1 or end == 0:\n",
    "                raise ValueError(\"No JSON object found\")\n",
    "            \n",
    "            data = json.loads(raw_judgment[start:end])\n",
    "            \n",
    "            # Convert anonymous IDs back to real model names\n",
    "            anonymous_to_model = {v: k for k, v in model_mapping.items()}\n",
    "            results = {}\n",
    "            \n",
    "            for anon_id, score in data.items():\n",
    "                model = anonymous_to_model.get(anon_id)\n",
    "                if not model:\n",
    "                    continue\n",
    "                    \n",
    "                if self.config.evaluation_method == 1:\n",
    "                    # Numeric scores\n",
    "                    score = float(score)\n",
    "                    score = max(1.0, min(10.0, score))\n",
    "                else:\n",
    "                    # Rankings\n",
    "                    score = int(score)\n",
    "                \n",
    "                results[model] = score\n",
    "                \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing evaluation: {str(e)}\")\n",
    "            # Return neutral scores\n",
    "            return {\n",
    "                model: 5.0 if self.config.evaluation_method == 1 else len(model_mapping)\n",
    "                for model in model_mapping.keys()\n",
    "            }\n",
    "\n",
    "    def evaluate_responses(self, \n",
    "                         responses_df: pd.DataFrame,\n",
    "                         llm_module) -> Tuple[nx.DiGraph, pd.DataFrame]:\n",
    "        \"\"\"Evaluate all responses and build the graph.\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(self.config.model_names)\n",
    "        \n",
    "        evaluations = []\n",
    "        \n",
    "        for prompt in responses_df['prompt'].unique():\n",
    "            prompt_responses = responses_df[\n",
    "                responses_df['prompt'] == prompt\n",
    "            ].set_index('model')['response'].to_dict()\n",
    "            \n",
    "            # For each judge model\n",
    "            for judge_model in self.config.model_names:\n",
    "                # Select models to evaluate\n",
    "                other_models = [\n",
    "                    m for m in self.config.model_names \n",
    "                    if m != judge_model and prompt_responses.get(m) is not None\n",
    "                ]\n",
    "                \n",
    "                if self.config.use_subset_evaluation:\n",
    "                    other_models = random.sample(\n",
    "                        other_models,\n",
    "                        min(self.config.evaluators_subset_size, len(other_models))\n",
    "                    )\n",
    "                \n",
    "                if not other_models:\n",
    "                    continue\n",
    "                \n",
    "                # Create evaluation prompt\n",
    "                eval_prompt, model_mapping = self._create_evaluation_prompt(\n",
    "                    prompt,\n",
    "                    {m: prompt_responses[m] for m in other_models}\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    # Get evaluation from judge\n",
    "                    raw_judgment = llm_module.get_model(judge_model).prompt(eval_prompt).text()\n",
    "                    parsed_judgments = self._parse_evaluation(raw_judgment, model_mapping)\n",
    "                    \n",
    "                    # Record evaluations\n",
    "                    for rated_model, score in parsed_judgments.items():\n",
    "                        evaluations.append({\n",
    "                            'prompt': prompt,\n",
    "                            'judge_model': judge_model,\n",
    "                            'rated_model': rated_model,\n",
    "                            'score': score\n",
    "                        })\n",
    "                        \n",
    "                        # Update graph\n",
    "                        if G.has_edge(judge_model, rated_model):\n",
    "                            G[judge_model][rated_model]['weight'] += score\n",
    "                        else:\n",
    "                            G.add_edge(judge_model, rated_model, weight=score)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during evaluation: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        return G, pd.DataFrame(evaluations)\n",
    "\n",
    "class SlopRank:\n",
    "    \"\"\"Main class for running the evaluation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvalConfig = DEFAULT_CONFIG):\n",
    "        self.config = config\n",
    "        self.response_manager = ResponseManager(config)\n",
    "        self.evaluator = Evaluator(config)\n",
    "        \n",
    "    def run(self, prompts: List[str], llm_module) -> Dict:\n",
    "        \"\"\"Run the full evaluation pipeline.\"\"\"\n",
    "        # Collect responses\n",
    "        logger.info(\"Collecting responses...\")\n",
    "        responses_df = self.response_manager.collect_responses(prompts, llm_module)\n",
    "        \n",
    "        # Save responses\n",
    "        responses_path = self.config.output_dir / \"responses.csv\"\n",
    "        responses_df.to_csv(responses_path, index=False)\n",
    "        logger.info(f\"Saved responses to {responses_path}\")\n",
    "        \n",
    "        # Evaluate responses\n",
    "        logger.info(\"Evaluating responses...\")\n",
    "        G, evaluations_df = self.evaluator.evaluate_responses(responses_df, llm_module)\n",
    "        \n",
    "        # Calculate PageRank\n",
    "        if len(G.edges) == 0:\n",
    "            logger.error(\"No valid evaluations to compute PageRank\")\n",
    "            return {}\n",
    "            \n",
    "        pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "        ranked_models = sorted(\n",
    "            pagerank_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            \"rankings\": ranked_models,\n",
    "            \"metadata\": {\n",
    "                \"evaluation_method\": self.config.evaluation_method,\n",
    "                \"use_subset_evaluation\": self.config.use_subset_evaluation,\n",
    "                \"evaluators_subset_size\": self.config.evaluators_subset_size,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save all artifacts\n",
    "        evaluations_df.to_csv(\n",
    "            self.config.output_dir / \"evaluations.csv\",\n",
    "            index=False\n",
    "        )\n",
    "        nx.write_gml(G, self.config.output_dir / \"endorsement_graph.gml\")\n",
    "        with open(self.config.output_dir / \"rankings.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "            \n",
    "        return results\n",
    "\n",
    "# %%\n",
    "def main():\n",
    "    \"\"\"Example usage of SlopRank.\"\"\"\n",
    "    import llm  # Your LLM module import\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    \n",
    "    logger.info(\"Starting SlopRank evaluation\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Read prompts\n",
    "        logger.info(\"Reading prompts from prompts.csv\")\n",
    "        prompts_df = pd.read_csv(\"prompts.csv\")\n",
    "        prompts = prompts_df[\"Questions\"].tolist()\n",
    "        # prompts = [prompts_df[\"Questions\"].iloc[2]]  # Use only select prompts - for testing\n",
    "        logger.info(f\"Loaded {len(prompts)} prompts\")\n",
    "        \n",
    "        # Initialize evaluation\n",
    "        config = DEFAULT_CONFIG\n",
    "        logger.info(f\"Using configuration: {config}\")\n",
    "        evaluator = SlopRank(config)\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluator.run(prompts, llm)\n",
    "        \n",
    "        # Print results\n",
    "        if results:\n",
    "            print(\"\\n=== Model Rankings ===\")\n",
    "            max_score = max(score for _, score in results[\"rankings\"])\n",
    "            for model, score in results[\"rankings\"]:\n",
    "                normalized_score = score / max_score * 10  # Normalize to 0-10 scale\n",
    "                print(f\"{model:30} {score:.4f} (normalized: {normalized_score:.2f})\")\n",
    "                \n",
    "            total_time = time.time() - start_time\n",
    "            logger.info(f\"Evaluation completed in {total_time:.2f}s\")\n",
    "        else:\n",
    "            logger.error(\"No results generated\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Older implementation: SlopRank\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import llm\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##############################################################################\n",
    "# 1. Configuration\n",
    "##############################################################################\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    \"gemini-exp-1206\",\n",
    "    \"claude-3-5-sonnet-latest\",\n",
    "    \"claude-3-opus-latest\",\n",
    "    \"o1-preview\",\n",
    "    \"gpt-4o\",\n",
    "    \"deepseek-chat\"\n",
    "]\n",
    "\n",
    "model_objects = {m: llm.get_model(m) for m in MODEL_NAMES}\n",
    "\n",
    "EVALUATION_METHOD = 1  # 1 (numeric 1â€“10) or 2 (Best-to-Worst)\n",
    "USE_SUBSET_EVALUATION = True  # Toggle to use partial evaluation\n",
    "EVALUATORS_SUBSET_SIZE = 3  # If True, limit judges to evaluate a subset of models\n",
    "\n",
    "##############################################################################\n",
    "# 2. Prompting functions\n",
    "##############################################################################\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a specified model via 'llm' and returns the response text.\n",
    "    \"\"\"\n",
    "    response = model_objects[model_name].prompt(prompt)\n",
    "    return response.text()\n",
    "\n",
    "def query_model_all(df, model_name):\n",
    "    \"\"\"\n",
    "    Query the chosen model for all prompts in the DataFrame and save responses.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    cleaned_prompts = df[\"prompt\"].str.strip().str.lower()\n",
    "    colname = f\"response_{model_name}\"\n",
    "    df[colname] = cleaned_prompts.map(lambda x: query_model(x, model_name))\n",
    "    print(f\"{model_name} processing time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    # Ensure the 'responses' directory exists\n",
    "    os.makedirs(\"responses\", exist_ok=True)\n",
    "    \n",
    "    # Save responses for this model\n",
    "    response_file_path = f\"responses/responses_{model_name}.csv\"\n",
    "    df[[colname]].to_csv(response_file_path, index=False)\n",
    "    print(f\"Saved responses for {model_name} to {response_file_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gather_all_model_responses(raw_prompts):\n",
    "    \"\"\"\n",
    "    Gather responses from all models and save to disk incrementally.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"prompt\": raw_prompts})\n",
    "    for m in MODEL_NAMES:\n",
    "        df = query_model_all(df, m)\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# 3. Evaluate the responses\n",
    "##############################################################################\n",
    "\n",
    "def create_evaluation_mapping(other_models):\n",
    "    \"\"\"\n",
    "    Creates a mapping between model names and anonymous identifiers.\n",
    "    Returns both forward and reverse mappings.\n",
    "    \"\"\"\n",
    "    model_to_anonymous = {model: f\"Model_{i+1}\" for i, model in enumerate(other_models)}\n",
    "    anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "    return model_to_anonymous, anonymous_to_model\n",
    "\n",
    "def build_evaluation_prompt(method, prompt, judge_model, model_responses, other_models):\n",
    "    \"\"\"\n",
    "    Build a meta-prompt.\n",
    "    \"\"\"\n",
    "    model_to_anonymous, _ = create_evaluation_mapping(other_models)\n",
    "    \n",
    "    answers_section = \"\\n\".join([\n",
    "        f\"{model_to_anonymous[om]}:\\n{model_responses[om]}\\n---\" \n",
    "        for om in other_models\n",
    "    ])\n",
    "\n",
    "    if method == 1:\n",
    "        instructions = f\"\"\"IMPORTANT: You must ONLY return a JSON object with ratings. No explanation or additional text.\n",
    "\n",
    "        You are an impartial judge able to analyse multiple responses to a prompt and give it an objective rating.\n",
    "\n",
    "        PROMPT TO EVALUATE:\n",
    "        \"{prompt}\"\n",
    "\n",
    "        RESPONSES TO RATE:\n",
    "        {answers_section}\n",
    "\n",
    "        RATING INSTRUCTIONS:\n",
    "        - Rate each response from 1 to 10\n",
    "        - Consider: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "        - 10: Exceptional, 8-9: Excellent, 6-7: Good, 4-5: Fair, 1-3: Poor\n",
    "\n",
    "        YOUR RESPONSE MUST BE EXACTLY IN THIS FORMAT:\n",
    "        {{\n",
    "            \"Model_1\": 8,\n",
    "            \"Model_2\": 7\n",
    "        }}\n",
    "\n",
    "        DO NOT include any other text, explanations, or analysis. ONLY the JSON object above.\"\"\"\n",
    "    else:\n",
    "        instructions = f\"\"\"IMPORTANT: You must ONLY return a JSON object with rankings. No explanation or additional text.\n",
    "\n",
    "        You are an impartial judge who will rank the given responses to a question objectvely.\n",
    "\n",
    "        PROMPT TO EVALUATE:\n",
    "        \"{prompt}\"\n",
    "\n",
    "        RESPONSES TO RANK:\n",
    "        {answers_section}\n",
    "\n",
    "        RANKING INSTRUCTIONS:\n",
    "        - Rank all responses from best to worst (1 = best, higher numbers = worse).\n",
    "        - Consider: accuracy, completeness, clarity, relevance, depth, usefulness.\n",
    "        - Do NOT assign the same rank to multiple responses (no ties).\n",
    "\n",
    "        YOUR RESPONSE MUST BE EXACTLY IN THIS FORMAT:\n",
    "        {{\n",
    "            \"Model_1\": 1,\n",
    "            \"Model_2\": 2,\n",
    "            \"Model_3\": 3\n",
    "        }}\n",
    "\n",
    "        DO NOT include any other text, explanations, or analysis. ONLY the JSON object above.\"\"\"\n",
    "\n",
    "    return instructions.strip(), model_to_anonymous\n",
    "\n",
    "def parse_evaluation_output(method, raw_judgment, anonymous_mapping):\n",
    "    \"\"\"\n",
    "    Enhanced parsing with better error handling and logging.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the input text more aggressively\n",
    "        cleaned_text = raw_judgment.strip()\n",
    "        # Find the first { and last }\n",
    "        start = cleaned_text.find(\"{\")\n",
    "        end = cleaned_text.rfind(\"}\") + 1\n",
    "        \n",
    "        if start == -1 or end == 0:\n",
    "            raise ValueError(\"No JSON object found in response\")\n",
    "            \n",
    "        json_str = cleaned_text[start:end]\n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Warning: Failed to parse judgment. Error: {str(e)}\")\n",
    "        print(f\"Raw response: {raw_judgment[:200]}...\")\n",
    "        # Return neutral fallback values\n",
    "        return {anonymous_mapping[k]: (5.0 if method == 1 else 0) for k in anonymous_mapping}\n",
    "\n",
    "    endorsement_map = {}\n",
    "    for anonymous_id, real_model in anonymous_mapping.items():\n",
    "        val = data.get(anonymous_id)\n",
    "        \n",
    "        if val is None:\n",
    "            print(f\"Warning: Missing rating for {anonymous_id}\")\n",
    "            endorsement_map[real_model] = 5.0 if method == 1 else 0\n",
    "            continue\n",
    "\n",
    "        if method == 1:\n",
    "            try:\n",
    "                score = float(val)\n",
    "                if not (1 <= score <= 10):\n",
    "                    print(f\"Warning: Score {score} for {anonymous_id} out of range, clamping to [1,10]\")\n",
    "                endorsement_map[real_model] = max(1.0, min(10.0, score))\n",
    "            except (TypeError, ValueError):\n",
    "                print(f\"Warning: Invalid numeric score for {anonymous_id}: {val}\")\n",
    "                endorsement_map[real_model] = 5.0\n",
    "        else:\n",
    "            if val is None:\n",
    "                print(f\"Warning: Missing rank for {anonymous_id}\")\n",
    "                endorsement_map[real_model] = len(anonymous_mapping)  # Assign worst rank\n",
    "            else:\n",
    "                try:\n",
    "                    endorsement_map[real_model] = int(val)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid rank for {anonymous_id}: {val}\")\n",
    "                    endorsement_map[real_model] = len(anonymous_mapping)  # Assign worst rank\n",
    "\n",
    "    return endorsement_map\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    \"\"\"\n",
    "    Evaluate responses with improved error handling and DataFrame operations.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(MODEL_NAMES)\n",
    "\n",
    "    # Initialize DataFrame to store evaluations\n",
    "    evaluations_df = pd.DataFrame({\n",
    "        'prompt': [],\n",
    "        'judge_model': [],\n",
    "        'rated_model_anonymous': [],\n",
    "        'rated_model_real': [],\n",
    "        'rating': [],\n",
    "        'method': []\n",
    "    })\n",
    "\n",
    "    # Filter valid evaluations only\n",
    "    valid_evaluations = evaluations_df[evaluations_df['rating'].notnull()]\n",
    "    \n",
    "    if valid_evaluations.empty:\n",
    "        print(\"No valid evaluations found. Skipping PageRank calculation.\")\n",
    "        return G, []\n",
    "\n",
    "    # Iterate through prompts and evaluate responses\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        model_responses = {m: row.get(f\"response_{m}\", \"No response\") for m in MODEL_NAMES}\n",
    "\n",
    "        for judge_model in MODEL_NAMES:\n",
    "            other_models = [m for m in MODEL_NAMES if m != judge_model]\n",
    "\n",
    "            # Use subset evaluation if enabled\n",
    "            if USE_SUBSET_EVALUATION and len(other_models) > EVALUATORS_SUBSET_SIZE:\n",
    "                other_models = random.sample(other_models, EVALUATORS_SUBSET_SIZE)\n",
    "\n",
    "            # Skip if no valid models to evaluate\n",
    "            if not other_models:\n",
    "                print(f\"Skipping evaluation for prompt: {prompt} (insufficient valid responses)\")\n",
    "                continue\n",
    "\n",
    "            # Build evaluation prompt\n",
    "            evaluation_prompt, model_to_anonymous = build_evaluation_prompt(\n",
    "                EVALUATION_METHOD, prompt, judge_model, model_responses, other_models\n",
    "            )\n",
    "            anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "\n",
    "            # Query the judge model\n",
    "            raw_judgment = query_model(evaluation_prompt, judge_model)\n",
    "            parsed_judgments = parse_evaluation_output(\n",
    "                EVALUATION_METHOD, raw_judgment, anonymous_to_model\n",
    "            )\n",
    "\n",
    "            # Create a new DataFrame for this batch of evaluations\n",
    "            new_evaluations = []\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                anonymous_id = model_to_anonymous[rated_model]\n",
    "                new_evaluations.append({\n",
    "                    'prompt': prompt,\n",
    "                    'judge_model': judge_model,\n",
    "                    'rated_model_anonymous': anonymous_id,\n",
    "                    'rated_model_real': rated_model,\n",
    "                    'rating': endorsement_val,\n",
    "                    'method': EVALUATION_METHOD\n",
    "                })\n",
    "            \n",
    "            # Concatenate efficiently\n",
    "            if new_evaluations:\n",
    "                evaluations_df = pd.concat([\n",
    "                    evaluations_df, \n",
    "                    pd.DataFrame(new_evaluations)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            # Update graph with valid ratings\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                if G.has_edge(judge_model, rated_model):\n",
    "                    G[judge_model][rated_model][\"weight\"] += endorsement_val\n",
    "                else:\n",
    "                    G.add_edge(judge_model, rated_model, weight=endorsement_val)\n",
    "\n",
    "    # Compute PageRank only if the graph has valid edges\n",
    "    if len(G.edges) == 0:\n",
    "        print(\"Graph has no valid edges. Cannot compute PageRank.\")\n",
    "        return G, []\n",
    "\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save all data\n",
    "    nx.write_gml(G, \"responses/endorsement_graph.gml\")\n",
    "    print(\"Saved endorsement graph to endorsement_graph.gml\")\n",
    "\n",
    "    evaluations_df.to_csv(\"responses/evaluations_with_mapping.csv\", index=False)\n",
    "    print(\"Saved detailed evaluations with mappings to evaluations_with_mapping.csv\")\n",
    "\n",
    "    with open(\"rankings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rankings\": ranked_models,\n",
    "            \"metadata\": {\n",
    "                \"evaluation_method\": EVALUATION_METHOD,\n",
    "                \"use_subset_evaluation\": USE_SUBSET_EVALUATION,\n",
    "                \"evaluators_subset_size\": EVALUATORS_SUBSET_SIZE,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "        }, f, indent=4)\n",
    "    print(\"Saved rankings to rankings.json\")\n",
    "\n",
    "    return G, ranked_models\n",
    "\n",
    "##############################################################################\n",
    "# 4. Main\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts_df = pd.read_csv(\"prompts.csv\")  # Assuming the CSV has a column named \"Questions\"\n",
    "    raw_prompts = prompts_df[\"Questions\"].tolist()\n",
    "    # raw_prompts = [prompts_df[\"Questions\"].iloc[2]]  # Use only select prompts - for testing\n",
    "\n",
    "    # Gather responses for each model\n",
    "    df_responses = gather_all_model_responses(raw_prompts)\n",
    "\n",
    "    # Evaluate responses and compute rankings\n",
    "    G, ranked = evaluate_responses(df_responses)\n",
    "\n",
    "    print(\"\\n=== PageRank Scores ===\")\n",
    "    for model, score in ranked:\n",
    "        print(f\"{model}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
